name: RAID Security Framework CI

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run security scans daily at 2 AM UTC
    - cron: '0 2 * * *'

env:
  PYTHON_VERSION: '3.11'
  DOCKER_BUILDKIT: 1
  COMPOSE_DOCKER_CLI_BUILD: 1

jobs:
  # Security and code quality checks
  security-lint:
    name: Security & Code Quality
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install bandit semgrep safety

      - name: Code formatting check
        run: |
          black --check --diff .
          ruff check .

      - name: Type checking
        run: mypy controller/ mcp/ ui/ --ignore-missing-imports

      - name: Security scanning with bandit
        run: bandit -r controller/ mcp/ ui/ -f json -o bandit-report.json
        continue-on-error: true

      - name: Security scanning with semgrep
        run: |
          python -m semgrep --config=auto --json --output=semgrep-report.json controller/ mcp/ ui/
        continue-on-error: true

      - name: Dependency vulnerability check
        run: safety check --json --output safety-report.json
        continue-on-error: true

      - name: Upload security reports
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: security-reports
          path: |
            bandit-report.json
            semgrep-report.json
            safety-report.json

  # Schema validation tests
  schema-validation:
    name: Schema Validation Tests
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Generate JSON schemas
        run: python specs/json-schemas.py

      - name: Validate schema generation
        run: |
          python -c "
          from specs.schemas import validate_assessment_plan, validate_authorization
          import json

          # Test plan validation
          test_plan = {
            'run_id': 'test-123',
            'plan_version': '1.0',
            'role': 'test-role',
            'target': 'example.com',
            'assessment_type': 'security_assessment',
            'phases': [],
            'total_estimated_runtime': 60,
            'llm_model': 'test-model'
          }

          try:
            validate_assessment_plan(test_plan)
            print('✅ Plan schema validation passed')
          except Exception as e:
            print(f'❌ Plan schema validation failed: {e}')
            exit(1)
          "

      - name: Upload generated schemas
        uses: actions/upload-artifact@v3
        with:
          name: json-schemas
          path: specs/json-schemas/

  # Unit tests
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.9', '3.10', '3.11', '3.12']
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest-cov pytest-xdist

      - name: Create test directories
        run: |
          mkdir -p tests/unit tests/integration tests/security
          mkdir -p results evidence auth roles secrets

      - name: Create basic unit tests
        run: |
          cat > tests/unit/test_schemas.py << 'EOF'
          """Test schema validation"""
          import pytest
          from specs.schemas import validate_assessment_plan, Authorization, AssessmentPlan
          from datetime import datetime

          def test_authorization_schema():
              auth_data = {
                  "auth_id": "test-001",
                  "issued_by": "test-issuer",
                  "issued_at": datetime.now(),
                  "expires_at": datetime.now(),
                  "allow_destructive": False,
                  "allowed_roles": ["web-pentest"],
                  "scope": {
                      "target_cidrs": ["203.0.113.0/24"],
                      "target_domains": ["example.com"]
                  },
                  "limits": {
                      "max_duration_hours": 24,
                      "max_tools_concurrent": 10,
                      "rate_limit_per_minute": 100,
                      "max_evidence_size_mb": 1000
                  },
                  "purpose": "Test assessment",
                  "contact_email": "test@example.com"
              }

              auth = Authorization(**auth_data)
              assert auth.auth_id == "test-001"
              assert not auth.allow_destructive

          def test_plan_validation():
              plan_data = {
                  "run_id": "test-run-123",
                  "plan_version": "1.0",
                  "role": "web-pentest",
                  "target": "example.com",
                  "assessment_type": "security_assessment",
                  "phases": [],
                  "total_estimated_runtime": 300,
                  "llm_model": "test-model"
              }

              plan = validate_assessment_plan(plan_data)
              assert plan.run_id == "test-run-123"
              assert plan.role == "web-pentest"
          EOF

          cat > tests/unit/test_llm_adapter.py << 'EOF'
          """Test LLM adapter functionality"""
          import pytest
          import asyncio
          from controller.llm_adapter import MockLLMAdapter, ValidationResult

          @pytest.mark.asyncio
          async def test_mock_llm_adapter():
              adapter = MockLLMAdapter()

              response = await adapter.generate_plan(
                  role="web-pentest",
                  target="example.com",
                  role_definition={"name": "web-pentest"},
                  authorization={"allow_destructive": False}
              )

              assert response.validation_result == ValidationResult.VALID
              assert response.total_tokens > 0
              assert "mock_run_" in response.content

          @pytest.mark.asyncio
          async def test_mock_tool_synthesis():
              adapter = MockLLMAdapter()

              response = await adapter.synthesize_tool(
                  requirements="Check HTTP headers",
                  context={"target": "example.com"}
              )

              assert response.validation_result == ValidationResult.VALID
              assert "custom_tool" in response.content
          EOF

          cat > tests/unit/test_network_isolation.py << 'EOF'
          """Test network isolation functionality"""
          import pytest
          from controller.network_isolation import NetworkPolicyBuilder, create_web_assessment_policy

          def test_network_policy_builder():
              policy = (NetworkPolicyBuilder("test-001")
                       .allow_cidr("203.0.113.0/24")
                       .block_private_networks()
                       .allow_dns(True)
                       .set_rate_limit(200)
                       .build())

              assert policy.network_id == "test-001"
              assert "203.0.113.0/24" in policy.allowed_cidrs
              assert "10.0.0.0/8" in policy.blocked_cidrs
              assert policy.allow_dns is True
              assert policy.rate_limit_per_minute == 200

          def test_web_assessment_policy():
              policy = create_web_assessment_policy(
                  network_id="web-test",
                  target_cidrs=["203.0.113.0/24"]
              )

              assert policy.network_id == "web-test"
              assert policy.allow_dns is True
              assert len(policy.allowed_cidrs) > 0
          EOF

      - name: Run unit tests with coverage
        run: |
          python -m pytest tests/unit/ -v --cov=controller --cov=mcp --cov=ui --cov-report=xml --cov-report=html

      - name: Upload coverage reports
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella

  # Docker and container tests
  docker-tests:
    name: Docker Build & Security Tests
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Create test Dockerfiles
        run: |
          mkdir -p docker/test

          cat > docker/test/Dockerfile.controller << 'EOF'
          FROM python:3.11-alpine

          # Security: Create non-root user
          RUN addgroup -g 1000 raid && \
              adduser -D -s /bin/sh -u 1000 -G raid raid

          WORKDIR /app
          COPY requirements.txt .
          RUN pip install --no-cache-dir -r requirements.txt

          COPY . .
          RUN chown -R raid:raid /app

          USER raid
          EXPOSE 8080
          CMD ["python", "-m", "controller.main"]
          EOF

          cat > docker/test/Dockerfile.mcp << 'EOF'
          FROM python:3.11-alpine

          RUN addgroup -g 1000 mcp && \
              adduser -D -s /bin/sh -u 1000 -G mcp mcp

          WORKDIR /app
          COPY requirements.txt .
          RUN pip install --no-cache-dir -r requirements.txt

          COPY . .
          RUN chown -R mcp:mcp /app

          USER mcp
          EXPOSE 8000
          CMD ["python", "-m", "mcp.server"]
          EOF

      - name: Build test images
        run: |
          docker build -f docker/test/Dockerfile.controller -t raid-controller:test .
          docker build -f docker/test/Dockerfile.mcp -t raid-mcp:test .

      - name: Run container security scan
        run: |
          # Install trivy for container scanning
          curl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sh -s -- -b /usr/local/bin

          # Scan images for vulnerabilities
          trivy image --exit-code 0 --format json --output controller-scan.json raid-controller:test
          trivy image --exit-code 0 --format json --output mcp-scan.json raid-mcp:test

      - name: Generate security profiles
        run: |
          python scripts/security_profiles.py
          ls -la docker/security/

      - name: Test container with security profiles
        run: |
          # Test that containers can start with security restrictions
          docker run --rm --security-opt=no-new-privileges --cap-drop=ALL --read-only --tmpfs=/tmp raid-controller:test python --version
          docker run --rm --security-opt=no-new-privileges --cap-drop=ALL --read-only --tmpfs=/tmp raid-mcp:test python --version

      - name: Upload container scan results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: container-scans
          path: |
            controller-scan.json
            mcp-scan.json
            docker/security/

  # Tool synthesis security tests
  synthesis-security:
    name: Tool Synthesis Security Tests
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Create synthesis security test
        run: |
          mkdir -p tests/security

          cat > tests/security/test_synthesis_pipeline.py << 'EOF'
          """Test tool synthesis security pipeline"""
          import pytest
          import tempfile
          import os
          from pathlib import Path

          def test_forbidden_imports_detection():
              """Test that forbidden imports are detected"""
              malicious_code = '''
          import os
          import subprocess

          def malicious_function():
              os.system("rm -rf /")
              subprocess.run(["curl", "evil.com"])
          '''

              # This would be caught by static analysis
              forbidden_patterns = ["os.system", "subprocess.run", "eval", "exec"]

              for pattern in forbidden_patterns:
                  assert pattern in malicious_code or pattern in ["eval", "exec"]

          def test_seccomp_profile_exists():
              """Test that seccomp profiles are generated"""
              from scripts.security_profiles import SecurityProfileGenerator

              with tempfile.TemporaryDirectory() as tmpdir:
                  generator = SecurityProfileGenerator(tmpdir)
                  generator.generate_synthesized_tool_seccomp()

                  seccomp_file = Path(tmpdir) / "synthesized-tool-seccomp.json"
                  assert seccomp_file.exists()

          def test_network_isolation_validation():
              """Test network isolation validation"""
              from controller.network_isolation import NetworkIsolationManager, create_isolated_policy

              manager = NetworkIsolationManager()
              policy = create_isolated_policy("test-isolated")

              # Test that isolation policy blocks network access
              assert not policy.allow_dns
              assert len(policy.allowed_cidrs) == 0
          EOF

      - name: Run synthesis security tests
        run: python -m pytest tests/security/ -v

      - name: Test security profile generation
        run: |
          python scripts/security_profiles.py

          # Verify profiles were created
          test -f docker/security/controller-seccomp.json
          test -f docker/security/tool-runner-seccomp.json
          test -f docker/security/synthesized-tool-seccomp.json
          test -f docker/security/raid-controller
          test -f docker/security/raid-tool-runner
          test -f docker/security/raid-synthesized-tool

  # Integration tests with mock services
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    services:
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Create integration tests
        run: |
          mkdir -p tests/integration

          cat > tests/integration/test_end_to_end.py << 'EOF'
          """End-to-end integration tests"""
          import pytest
          import tempfile
          import asyncio
          from pathlib import Path
          from controller.llm_adapter import MockLLMAdapter
          from specs.schemas import validate_assessment_plan

          @pytest.mark.asyncio
          async def test_plan_generation_flow():
              """Test complete plan generation flow"""
              adapter = MockLLMAdapter()

              # Test plan generation
              response = await adapter.generate_plan(
                  role="web-pentest",
                  target="example.com",
                  role_definition={"tools": ["http-fetcher"]},
                  authorization={"allow_destructive": False, "scope": {"target_cidrs": ["0.0.0.0/0"]}}
              )

              assert response.validation_result.value == "valid"

              # Validate the generated plan
              import json
              plan_data = json.loads(response.content)
              plan = validate_assessment_plan(plan_data)

              assert plan.role == "web-pentest"
              assert plan.target == "example.com"
              assert len(plan.phases) > 0

          def test_security_profile_integration():
              """Test security profile integration"""
              from scripts.security_profiles import SecurityProfileGenerator

              with tempfile.TemporaryDirectory() as tmpdir:
                  generator = SecurityProfileGenerator(tmpdir)
                  generator.generate_all_profiles()

                  # Verify all profiles created
                  profiles_dir = Path(tmpdir)
                  assert (profiles_dir / "controller-seccomp.json").exists()
                  assert (profiles_dir / "raid-controller").exists()
                  assert (profiles_dir / "syscall_monitor.py").exists()
          EOF

      - name: Run integration tests
        run: python -m pytest tests/integration/ -v

  # Device security check test
  device-security:
    name: Device Security Check Test
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Test device security checker (dry run)
        run: |
          # Test device security checker with non-existent device (should fail gracefully)
          python scripts/device_security_check.py /dev/null || echo "Expected failure for invalid device"

          # Check that security report was generated
          test -d /tmp/raid-device-checks

      - name: Test emergency shutdown (dry run)
        run: |
          # Test emergency shutdown components (without actually running)
          python -c "
          from scripts.emergency_shutdown import EmergencyShutdown
          shutdown = EmergencyShutdown()
          print('Emergency shutdown components initialized successfully')
          "

  # Final status check
  status-check:
    name: CI Status Summary
    runs-on: ubuntu-latest
    needs: [security-lint, schema-validation, unit-tests, docker-tests, synthesis-security, integration-tests, device-security]
    if: always()
    steps:
      - name: Check overall status
        run: |
          echo "CI Pipeline Results Summary:"
          echo "Security & Lint: ${{ needs.security-lint.result }}"
          echo "Schema Validation: ${{ needs.schema-validation.result }}"
          echo "Unit Tests: ${{ needs.unit-tests.result }}"
          echo "Docker Tests: ${{ needs.docker-tests.result }}"
          echo "Synthesis Security: ${{ needs.synthesis-security.result }}"
          echo "Integration Tests: ${{ needs.integration-tests.result }}"
          echo "Device Security: ${{ needs.device-security.result }}"

          # Fail if any critical jobs failed
          if [[ "${{ needs.security-lint.result }}" == "failure" ||
                "${{ needs.schema-validation.result }}" == "failure" ||
                "${{ needs.synthesis-security.result }}" == "failure" ]]; then
            echo "❌ Critical security checks failed"
            exit 1
          else
            echo "✅ All critical checks passed"
          fi